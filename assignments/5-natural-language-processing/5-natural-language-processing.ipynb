{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a429067d13d64b3c91f285ed1aedc5eb","deepnote_cell_type":"markdown"},"source":["# Assignment 5 - Natural Language Processing\n","\n","- Student 1 - Luca Modica\n","- Student 2 - Hugo Alves Henriques E Silva\n","\n","---"]},{"cell_type":"code","execution_count":73,"metadata":{"cell_id":"a0b8ec62945f45d490d3dcc6efc9f49e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":852,"execution_start":1706548661380,"source_hash":"820f4cc4"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import math\n","\n","sns.set_style()\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"cell_id":"c75378b090bf4bb192cc08d6d1c5999c","deepnote_cell_type":"markdown"},"source":["## Reading data"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["from collections import Counter\n","import re\n","\n","# Paths to the files\n","de_file_path = 'dat410_europarl/europarl-v7.de-en.lc.de'\n","en_de_file_path = 'dat410_europarl/europarl-v7.de-en.lc.en'\n","fr_file_path = 'dat410_europarl/europarl-v7.fr-en.lc.fr'\n","en_fr_file_path = 'dat410_europarl/europarl-v7.fr-en.lc.en'\n","sv_file_path = 'dat410_europarl/europarl-v7.sv-en.lc.sv'\n","en_sv_file_path = 'dat410_europarl/europarl-v7.sv-en.lc.en'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Warmup"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["# Function to read a file and return word frequencies\n","def get_word_frequencies(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read().lower()  # Ensure all text is lowercase\n","        words = re.findall(r'\\b\\w+\\b', text)  # Extract words\n","        word_freq = Counter(words)  # Count word frequencies\n","    return word_freq\n"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Most common words in German: [('die', 10521), ('der', 9374), ('und', 7028), ('in', 4175), ('zu', 3169), ('den', 2976), ('wir', 2863), ('daß', 2738), ('ich', 2670), ('das', 2669)]\n","Most common words in English (German-English pair): [('the', 19853), ('of', 9633), ('to', 9069), ('and', 7307), ('in', 6278), ('is', 4478), ('that', 4441), ('a', 4438), ('we', 3372), ('this', 3362)]\n","Most common words in French: [('apos', 16729), ('de', 14528), ('la', 9746), ('et', 6620), ('l', 6536), ('le', 6177), ('à', 5588), ('les', 5587), ('des', 5232), ('que', 4797)]\n","Most common words in English (French-English pair): [('the', 19627), ('of', 9534), ('to', 8992), ('and', 7214), ('in', 6197), ('is', 4453), ('that', 4421), ('a', 4388), ('we', 3341), ('this', 3332)]\n","Most common words in Swedish: [('att', 9181), ('och', 7038), ('i', 5954), ('det', 5687), ('som', 5028), ('för', 4959), ('av', 4013), ('är', 3840), ('en', 3724), ('vi', 3211)]\n","Most common words in English (Swedish-English pair): [('the', 19327), ('of', 9344), ('to', 8814), ('and', 6949), ('in', 6124), ('is', 4400), ('that', 4357), ('a', 4271), ('we', 3223), ('this', 3222)]\n"]}],"source":["# Get word frequencies for German-English pair\n","de_word_freq = get_word_frequencies(de_file_path)\n","en_de_word_freq = get_word_frequencies(en_de_file_path)\n","\n","# Print the 10 most common words in German and English (German-English pair)\n","de_common_words = de_word_freq.most_common(10)\n","en_de_common_words = en_de_word_freq.most_common(10)\n","\n","# Get word frequencies for French-English pair\n","fr_word_freq = get_word_frequencies(fr_file_path)\n","en_fr_word_freq = get_word_frequencies(en_fr_file_path)\n","\n","# Get word frequencies for Swedish-English pair\n","sv_word_freq = get_word_frequencies(sv_file_path)\n","en_sv_word_freq = get_word_frequencies(en_sv_file_path)\n","\n","# Print the 10 most common words in French, English (French-English pair), Swedish, and English (Swedish-English pair)\n","fr_common_words = fr_word_freq.most_common(10)\n","en_fr_common_words = en_fr_word_freq.most_common(10)\n","sv_common_words = sv_word_freq.most_common(10)\n","en_sv_common_words = en_sv_word_freq.most_common(10)\n","\n","print(\"Most common words in German:\", de_common_words)\n","print(\"Most common words in English (German-English pair):\", en_de_common_words)\n","print(\"Most common words in French:\", fr_common_words)\n","print(\"Most common words in English (French-English pair):\", en_fr_common_words)\n","print(\"Most common words in Swedish:\", sv_common_words)\n","print(\"Most common words in English (Swedish-English pair):\", en_sv_common_words)"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words: 784458\n","Speaker count: 33\n","Zebra count: 0\n","Probability of 'speaker': 4.206726172720528e-05\n","Probability of 'zebra': 0.0\n"]}],"source":["# Calculate the total word counts and the counts for 'speaker' and 'zebra' across all English files\n","total_words = sum(en_de_word_freq.values()) + sum(en_fr_word_freq.values()) + sum(en_sv_word_freq.values())\n","speaker_count = en_de_word_freq['speaker'] + en_fr_word_freq['speaker'] + en_sv_word_freq['speaker']\n","zebra_count = en_de_word_freq['zebra'] + en_fr_word_freq['zebra'] + en_sv_word_freq['zebra']\n","\n","# Calculate probabilities\n","prob_speaker = speaker_count / total_words\n","prob_zebra = zebra_count / total_words\n","\n","print(\"Total words:\", total_words)\n","print(\"Speaker count:\", speaker_count)\n","print(\"Zebra count:\", zebra_count)\n","print(\"Probability of 'speaker':\", prob_speaker)\n","print(\"Probability of 'zebra':\", prob_zebra)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Language modeling"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Probability of ('<START>', 'the') : 0.11426666666666667\n","Probability of ('the', 'zebra') : 0.0\n"]}],"source":["from nltk.tokenize import word_tokenize\n","from nltk.util import bigrams\n","\n","# Function to tokenize corpus into bigrams with start and end tokens\n","def create_bigrams(text):\n","    sentences = text.split('\\n')\n","    bigram_list = []\n","    for sentence in sentences:\n","        tokens = ['<START>'] + word_tokenize(sentence)\n","        bigram_list.extend(list(bigrams(tokens)))\n","    return bigram_list\n","\n","# Read the English text files from all three pairs to create a single corpus\n","corpus_de_en = open(en_de_file_path, 'r', encoding='utf-8').read()\n","corpus_fr_en = open(en_fr_file_path, 'r', encoding='utf-8').read()\n","corpus_sv_en = open(en_sv_file_path, 'r', encoding='utf-8').read()\n","\n","# Combine the corpora\n","combined_corpus = '\\n'.join([corpus_de_en, corpus_fr_en, corpus_sv_en])\n","\n","# Create bigrams from the combined corpus\n","bigram_list = create_bigrams(combined_corpus)\n","\n","# Calculate bigram and unigram counts\n","unigram_counts = Counter([unigram for bigram in bigram_list for unigram in bigram])\n","bigram_counts = Counter(bigram_list)\n","\n","# Function to calculate bigram probabilities using MLE\n","def calculate_bigram_prob(bigram):\n","    return bigram_counts[bigram] / unigram_counts[bigram[0]]\n","\n","# Test the function with an example bigram\n","example_bigram = ('<START>', 'the')\n","print(\"Probability of\", example_bigram, \":\", calculate_bigram_prob(example_bigram))\n","example_bigram = ('the', 'zebra')\n","print(\"Probability of\", example_bigram, \":\", calculate_bigram_prob(example_bigram))"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Probability of \"why are no-smoking areas not enforced ?\": 7.157743716417319e-18\n","Probability of \"the door is green\": 0.0\n","Probability of \"we pass\": 2.0531400966183576e-05\n"]}],"source":["def calculate_sentence_prob(sentence):\n","    sentence_bigram_list = create_bigrams(sentence)\n","    probability = 1\n","    for bigram in sentence_bigram_list:\n","        probability *= calculate_bigram_prob(bigram)\n","    return probability\n","\n","\n","print(\n","    f'Probability of \"why are no-smoking areas not enforced ?\": {calculate_sentence_prob(\"why are no-smoking areas not enforced ?\")}')\n","print(\n","    f'Probability of \"the door is green\": {calculate_sentence_prob(\"the door is green\")}')\n","print(\n","    f'Probability of \"we pass\": {calculate_sentence_prob(\"we pass\")}')"]},{"cell_type":"markdown","metadata":{},"source":["When we encounter a word that did not appear in the training texts, this will result in a probability of zero for any bigram containing this word, making the probability of the entire sentence zero. This is a common issue in language modeling known as the zero-probability problem, and it can be handled using techniques like Laplace (add-one) smoothing.\n","\n","If the sentence is very long, the probability of the sentence will tend to be very small due to the multiplication of probabilities, which can lead to underflow problems in computers. One way to handle this is by working with the log probabilities instead of the raw probabilities."]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Log probability of \"why are no-smoking areas not enforced ?\": -57.44415862256026\n","Log probability of \"the door is open\": -29.796067530994655\n","Log probability of \"the door is green\": -31.741977680049967\n","Log probability of \"we pass\": -11.416178508839266\n"]}],"source":["# Calculate the vocabulary size\n","vocabulary_size = len(unigram_counts)\n","\n","# Function to calculate bigram probabilities using Laplace smoothing\n","def calculate_bigram_log_prob_with_laplace(bigram):\n","    numerator = bigram_counts[bigram] + 1  # Add one to the count for Laplace smoothing\n","    denominator = unigram_counts[bigram[0]] + vocabulary_size  # Add vocabulary size for Laplace smoothing\n","    return math.log(numerator) - math.log(denominator)\n","\n","\n","#calculate probability of a sentence\n","def calculate_sentence_prob_improved(sentence):\n","    tokens = ['<START>'] + word_tokenize(sentence.lower())\n","    probability = 0\n","    for i in range(len(tokens) - 1):\n","        bigram = (tokens[i], tokens[i + 1])\n","        probability += calculate_bigram_log_prob_with_laplace(bigram)\n","    return probability\n","\n","\n","print(\n","    f'Log probability of \"why are no-smoking areas not enforced ?\": {calculate_sentence_prob_improved(\"why are no-smoking areas not enforced ?\")}')\n","print(\n","    f'Log probability of \"the door is open\": {calculate_sentence_prob_improved(\"the door is open\")}')\n","print(\n","    f'Log probability of \"the door is green\": {calculate_sentence_prob_improved(\"the door is green\")}')\n","print(\n","    f'Log probability of \"we pass\": {calculate_sentence_prob_improved(\"we pass\")}')\n"]},{"cell_type":"markdown","metadata":{},"source":["The more negative a log probability is, the less likely the sentence is."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Translation modeling"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['the', 'dog', 'runs'], ['the', 'cat', 'sleeps']]\n","{('the', '<NULL>'): 1e-05, ('the', 'le'): 0.199998, ('the', 'chien'): 0.199998, ('the', 'court'): 0.199998, ('the', 'chat'): 0.199998, ('the', 'dort'): 0.199998, ('dog', '<NULL>'): 1e-05, ('dog', 'le'): 0.33333, ('dog', 'chien'): 0.33333, ('dog', 'court'): 0.33333, ('runs', '<NULL>'): 1e-05, ('runs', 'le'): 0.33333, ('runs', 'chien'): 0.33333, ('runs', 'court'): 0.33333, ('cat', '<NULL>'): 1e-05, ('cat', 'le'): 0.33333, ('cat', 'chat'): 0.33333, ('cat', 'dort'): 0.33333, ('sleeps', '<NULL>'): 1e-05, ('sleeps', 'le'): 0.33333, ('sleeps', 'chat'): 0.33333, ('sleeps', 'dort'): 0.33333, ('i', '<NULL>'): 1e-05, ('i', 'je'): 0.499995, ('i', 'suis'): 0.499995, ('am', '<NULL>'): 1e-05, ('am', 'je'): 0.499995, ('am', 'suis'): 0.499995}\n"]}],"source":["import string\n","\n","def tokenize_corpus(corpus, add_null=False):\n","    \"\"\"Tokenize the input corpus (a list of sentences) into a list of lists of tokens.\n","    Optionally add a NULL token at the beginning of each sentence.\"\"\"\n","    clean_corpus = [sentence.translate(str.maketrans(\n","        '', '', string.punctuation)) for sentence in corpus]\n","    tokenized_corpus = [sentence.lower().split() for sentence in clean_corpus]\n","    if add_null:\n","        for sentence in tokenized_corpus:\n","            sentence.insert(0, \"<NULL>\")\n","    return tokenized_corpus\n","\n","\n","def initialize_translation_prob(corpus_english, corpus_foreign):\n","    \"\"\"Initialize translation probabilities with a lower probability for NULL.\"\"\"\n","\n","    word_correspondence = {}\n","\n","    for sentence_e, sentence_f in zip(corpus_english, corpus_foreign):\n","        for word_e in sentence_e:\n","            if word_e not in word_correspondence:\n","                word_correspondence[word_e] = []\n","            for word_f in sentence_f:\n","                if word_f not in word_correspondence[word_e]:\n","                    word_correspondence[word_e] += [word_f]\n","\n","\n","    translation_prob = {}\n","    null_prob = 0.00001\n","\n","    # print(corpus_foreign)\n","\n","\n","    for word_e in word_correspondence:\n","        for word_f in word_correspondence[word_e]:\n","            if word_f == \"<NULL>\":\n","               translation_prob[(word_e, word_f)] = null_prob\n","            else:\n","                translation_prob[(word_e, word_f)] = (1 - null_prob) / (len(word_correspondence[word_e]) - 1)\n","            #translation_prob[(word_e, word_f)] = 1 / len(word_correspondence[word_e])\n","\n","    return translation_prob\n","\n","\n","\n","\n","print(tokenize_corpus([\"The dog runs\", \"The cat sleeps\"]))\n","print(initialize_translation_prob(tokenize_corpus([\"The dog runs\", \"The cat sleeps\", \"I am\"]), tokenize_corpus([\"Le chien court\", \"Le chat dort\", \"Je suis\"], add_null=True)))"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","\n","def ibm_model_1(corpus_english, corpus_foreign, iterations=10):\n","    # Assuming tokenize_corpus adds a \"null\" token to the beginning of each English sentence\n","    # and splits sentences into lists of words.\n","    corpus_foreign_tokens = tokenize_corpus(corpus_foreign, add_null=True)  # foreign language corpus\n","    corpus_english_tokens = tokenize_corpus(corpus_english)  # English corpus, with null word\n","\n","    # Initialize translation probabilities uniformly\n","    translation_prob = initialize_translation_prob(corpus_english_tokens, corpus_foreign_tokens)\n","\n","    for iteration in range(iterations):\n","        count_ef = defaultdict(float)\n","        total_e = defaultdict(float)\n","        \n","        # E-step: Expectation\n","        for sentence_e, sentence_f in zip(corpus_english_tokens, corpus_foreign_tokens):\n","            # For each word in the english sentence\n","            for word_f in sentence_f:\n","                # Compute normalization factor for the word2\n","                s_total_word_e = sum(translation_prob[(word_e, word_f)] for word_e in sentence_e)\n","                # For each word in the foreign sentence\n","                for word_e in sentence_e:\n","                    # Calculate delta, which is the proportion of the alignment probability of the word2 to the word1\n","                    delta = translation_prob[(word_e, word_f)] / s_total_word_e\n","                    # Update counts\n","                    count_ef[(word_e, word_f)] += delta\n","                    total_e[word_e] += delta\n","        \n","        # M-step: Maximization\n","        for (word_e, word_f), count in count_ef.items():\n","            translation_prob[(word_e, word_f)] = count / total_e[word_e]\n","\n","\n","        # normalize probabilities\n","        new_dict = {}\n","        for key, value in translation_prob.items():\n","            if key[0] not in new_dict:\n","                new_dict[key[0]] = value\n","            else:\n","                new_dict[key[0]] += value\n","\n","        for key, value in translation_prob.items():\n","            translation_prob[key] = value / new_dict[key[0]]\n","\n","    return translation_prob\n","\n"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{('the', '<NULL>'): 1e-05, ('the', 'das'): 0.33333, ('the', 'haus'): 0.33333, ('the', 'buch'): 0.33333, ('house', '<NULL>'): 1e-05, ('house', 'das'): 0.2499975, ('house', 'haus'): 0.2499975, ('house', 'ein'): 0.2499975, ('house', 'großes'): 0.2499975, ('book', '<NULL>'): 1e-05, ('book', 'das'): 0.499995, ('book', 'buch'): 0.499995, ('a', '<NULL>'): 1e-05, ('a', 'ein'): 0.33333, ('a', 'großes'): 0.33333, ('a', 'haus'): 0.33333, ('big', '<NULL>'): 1e-05, ('big', 'ein'): 0.33333, ('big', 'großes'): 0.33333, ('big', 'haus'): 0.33333}\n","Top translations for 'house':\n","haus: 0.6693175783779879\n","<NULL>: 0.33068242162201217\n","das: 3.535814090616524e-27\n","ein: 2.6127305810792512e-33\n","großes: 2.6127305810792512e-33\n","1.0\n"]}],"source":["# Example usage (using dummy data):\n","corpus1 = [\"the house\", \"the book\", \"a big house\"]\n","corpus2 = [\"das haus\", \"das buch\", \"ein großes haus\"]  # Assuming German for demonstration\n","\n","print(initialize_translation_prob(tokenize_corpus(corpus1), tokenize_corpus(corpus2, add_null=True)))\n","\n","# Estimate translation probabilities\n","translation_prob = ibm_model_1(corpus1, corpus2, iterations=100)\n","\n","# Find translations for a specific word (e.g., \"house\")\n","translations_for_word = {pair[1]: prob for pair, prob in translation_prob.items() if pair[0] == \"house\"}\n","# Sort translations by probability\n","sorted_translations = sorted(translations_for_word.items(), key=lambda item: item[1], reverse=True)\n","\n","# Print top N translations\n","print(\"Top translations for 'house':\")\n","summm = 0\n","for foreign_word, prob in sorted_translations[:10]:\n","    print(f\"{foreign_word}: {prob}\")\n","    summm += prob\n","print(summm)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Need to be very careful with the NULL probability. It needs to be way lower than the other probabilities, otherwise the model will always choose the NULL translation, because NULL will be present in every sentence. Increasing the number of iterations will eventually make null the most probable translation for every word."]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["#Write code that implements the estimation algorithm for IBM model 1.\n","# Then print, for either Swedish, German, or French, the 10 words that \n","#the English word european is most likely to be translated into, according \n","#to your estimate. It can be interesting to look at this list of 10 words and\n","#see how it changes during the EM iterations.\n","\n","#reduce the size of corpus_de_en and corpus_sv_en\n","sv_en = open(en_sv_file_path, 'r', encoding='utf-8').read()\n","#separate the sentences\n","sv_en = sv_en.split(\"\\n\")\n","corpus_en = sv_en[:]\n","\n","sv = open(sv_file_path, 'r', encoding='utf-8').read()\n","#separate the sentences\n","sv = sv.split(\"\\n\")\n","corpus_sv = sv[:]\n","\n","#print(corpus_en)\n","#print(corpus_sv)\n","\n","# Estimate translation probabilities\n","translation_prob = ibm_model_1(corpus_en, corpus_sv, iterations=30)\n"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top translations for 'european' in Swedish:\n","europeisk: 0.3629289715804933\n","åklagare: 0.3338594041812997\n","åklagarmyndighet: 0.16503145376272627\n","allmän: 0.10058253866137612\n","åklagarmyndigheten: 0.03614154802863032\n","en: 0.0012240205256121347\n","skulle: 0.00021618874044551938\n","vara: 1.577447072652295e-05\n","inrättas: 5.525971338434354e-08\n","ha: 1.0693275441837174e-08\n","1.0000000000000002\n"]}],"source":["# Find translations for a specific word (e.g., \"european\")\n","translations_for_word = {pair[1]: prob for pair, prob in translation_prob.items() if pair[0] == \"prosecutor\"}\n","# Sort the top 10 translations by probability\n","\n","suma = 0\n","\n","for i in translations_for_word:\n","    suma = suma + translations_for_word[i]\n","swedish_translations = sorted(translations_for_word.items(), key=lambda item: item[1], reverse=True)[:10]\n","\n","print(\"Top translations for 'european' in Swedish:\")\n","\n","for foreign_word, prob in swedish_translations:\n","    print(f\"{foreign_word}: {prob}\")\n","\n","print(suma)"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Translated sentence: i am european\n"]}],"source":["def get_top_n_word_translations(foreign_sentence, translation_prob, n):\n","\n","    word_translations = {}\n","\n","    for word in foreign_sentence:\n","        #get top 5 translations for each word\n","        translations_for_word = {pair[0]: prob for pair, prob in translation_prob.items() if pair[1] == word}\n","        # Sort translations by probability\n","        sorted_translations = sorted(translations_for_word.items(), key=lambda item: item[1], reverse=True)[:n]\n","        #print(f\"Top translations for '{word}':\")\n","        #for foreign_word, prob in sorted_translations[:n]:\n","         #   print(f\"{foreign_word}: {prob}\")\n","        #print(\"\\n\")\n","\n","        word_translations[word] = sorted_translations\n","\n","    return word_translations\n","\n","\"\"\" for word in top_n_word_translations:\n","\n","        # get \n","\n","        for i in top_n_word_translations[word]:\n","    \n","            sentences.append(i[0])\n","\n","        translations.append(sentence)\n","\n","    print(\"Translations:\", translations)  \"\"\"  \n","\n","\n","def translate_sentence(foreign_sentence, top_n_word_translations):\n","\n","    translations = []\n","\n","    #make a combination of every single possibility, maintaining the order of the words\n","\n","    for word in foreign_sentence:\n","\n","        if word in top_n_word_translations:\n","\n","            if len(translations) == 0:\n","\n","                for translation in top_n_word_translations[word]:\n","\n","                    translations.append([translation])\n","\n","            else:\n","\n","                temp_translations = []\n","\n","                for sentence in translations:\n","\n","                    for translation in top_n_word_translations[word]:\n","\n","                        temp_translations.append(sentence + [translation])\n","\n","                translations = temp_translations\n","\n","\n","    #print(\"Translations:\", translations)\n","    #raise ValueError(\"\")\n","\n","    #for translation in translations:\n","        #print(translation)\n","\n","    highest_prob_sentence = []\n","    highest_prob = -10000000\n","\n","    for sentence in translations:\n","\n","        sentence_probability = calculate_sentence_prob_improved(\" \".join([word[0] for word in sentence]))\n","\n","        if sentence_probability > highest_prob:\n","            highest_prob = sentence_probability\n","            highest_prob_sentence = sentence\n","\n","    #print(\"Highest probability sentence:\", highest_prob_sentence)\n","\n","    return \" \".join([word[0] for word in highest_prob_sentence])\n","\n","\n","\n","\n","\n","sentence = \"jag är europeiska\"\n","\n","swedish_sentence = sentence.split()\n","\n","top_5_word_translations = get_top_n_word_translations(swedish_sentence, translation_prob, 5)\n","\n","#print(top_5_word_translations)\n","\n","translated_sentence = translate_sentence(swedish_sentence, top_5_word_translations)\n","\n","print(\"Translated sentence:\", translated_sentence)\n","\n","\n","#print the frist 10 elements of translation_prob\n","#for i in range(10):\n","    #print(list(translation_prob.items())[i])\n","\n","#jag förklarar europaparlamentets session återupptagen efter avbrottet den 17 december . jag vill på nytt önska er ett gott nytt år och jag hoppas att ni haft en trevlig semester .\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Decoding"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"a97e5a613ce44d2bbe2564741918b105","deepnote_persisted_session":{"createdAt":"2024-01-29T14:08:51.714Z"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"vscode":{"interpreter":{"hash":"8e21d9f50f7c30e7092d9e7e50ded244397ed7adf91fc30361913d3d8a47365c"}}},"nbformat":4,"nbformat_minor":0}
