{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a429067d13d64b3c91f285ed1aedc5eb","deepnote_cell_type":"markdown"},"source":["# Assignment 5 - Natural Language Processing\n","\n","- Student 1 - Luca Modica\n","- Student 2 - Hugo Alves Henriques E Silva\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a0b8ec62945f45d490d3dcc6efc9f49e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":852,"execution_start":1706548661380,"source_hash":"820f4cc4"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import math\n","\n","sns.set_style()\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"cell_id":"c75378b090bf4bb192cc08d6d1c5999c","deepnote_cell_type":"markdown"},"source":["## Reading data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import Counter\n","import re\n","\n","# Paths to the files\n","de_file_path = 'dat410_europarl/europarl-v7.de-en.lc.de'\n","en_de_file_path = 'dat410_europarl/europarl-v7.de-en.lc.en'\n","fr_file_path = 'dat410_europarl/europarl-v7.fr-en.lc.fr'\n","en_fr_file_path = 'dat410_europarl/europarl-v7.fr-en.lc.en'\n","sv_file_path = 'dat410_europarl/europarl-v7.sv-en.lc.sv'\n","en_sv_file_path = 'dat410_europarl/europarl-v7.sv-en.lc.en'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Warmup"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_word_frequencies(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read().lower() \n","        words = re.findall(r'\\b\\w+\\b', text)\n","        word_freq = Counter(words)\n","    return word_freq\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Get word frequencies for German-English pair\n","de_word_freq = get_word_frequencies(de_file_path)\n","en_de_word_freq = get_word_frequencies(en_de_file_path)\n","\n","# Print the 10 most common words in German and English (German-English pair)\n","de_common_words = de_word_freq.most_common(10)\n","en_de_common_words = en_de_word_freq.most_common(10)\n","\n","# Get word frequencies for French-English pair\n","fr_word_freq = get_word_frequencies(fr_file_path)\n","en_fr_word_freq = get_word_frequencies(en_fr_file_path)\n","\n","# Get word frequencies for Swedish-English pair\n","sv_word_freq = get_word_frequencies(sv_file_path)\n","en_sv_word_freq = get_word_frequencies(en_sv_file_path)\n","\n","# Print the 10 most common words in French, English (French-English pair), Swedish, and English (Swedish-English pair)\n","fr_common_words = fr_word_freq.most_common(10)\n","en_fr_common_words = en_fr_word_freq.most_common(10)\n","sv_common_words = sv_word_freq.most_common(10)\n","en_sv_common_words = en_sv_word_freq.most_common(10)\n","\n","print(\"Most common words in German:\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in de_common_words]))\n","\n","print(\"Most common words in English (German-English pair):\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in en_de_common_words]))\n","\n","print(\"Most common words in French:\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in fr_common_words]))\n","\n","print(\"Most common words in English (French-English pair):\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in en_fr_common_words]))\n","\n","print(\"Most common words in Swedish:\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in sv_common_words]))\n","\n","print(\"Most common words in English (Swedish-English pair):\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in en_sv_common_words]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the total word counts and the counts for 'speaker' and 'zebra' across all English files\n","total_words = sum(en_de_word_freq.values()) + sum(en_fr_word_freq.values()) + sum(en_sv_word_freq.values())\n","speaker_count = en_de_word_freq['speaker'] + en_fr_word_freq['speaker'] + en_sv_word_freq['speaker']\n","zebra_count = en_de_word_freq['zebra'] + en_fr_word_freq['zebra'] + en_sv_word_freq['zebra']\n","\n","# Calculate probabilities\n","prob_speaker = speaker_count / total_words\n","prob_zebra = zebra_count / total_words\n","\n","print(\"Total words:\", total_words)\n","print(\"Speaker count:\", speaker_count)\n","print(\"Zebra count:\", zebra_count)\n","print(\"Probability of 'speaker':\", prob_speaker)\n","print(\"Probability of 'zebra':\", prob_zebra)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Language modeling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","from nltk.util import bigrams\n","\n","# Function to tokenize corpus into bigrams with start and end tokens\n","def create_bigrams(text):\n","    sentences = text.split('\\n')\n","    bigram_list = []\n","    for sentence in sentences:\n","        tokens = ['<START>'] + word_tokenize(sentence)\n","        bigram_list.extend(list(bigrams(tokens)))\n","    return bigram_list\n","\n","# Read the English text files from all three pairs to create a single corpus\n","corpus_de_en = open(en_de_file_path, 'r', encoding='utf-8').read()\n","corpus_fr_en = open(en_fr_file_path, 'r', encoding='utf-8').read()\n","corpus_sv_en = open(en_sv_file_path, 'r', encoding='utf-8').read()\n","\n","# Combine the corpora\n","combined_corpus = '\\n'.join([corpus_de_en, corpus_fr_en, corpus_sv_en])\n","\n","# Create bigrams from the combined corpus\n","bigram_list = create_bigrams(combined_corpus)\n","\n","# Calculate bigram and unigram counts\n","unigram_counts = Counter([unigram for bigram in bigram_list for unigram in bigram])\n","bigram_counts = Counter(bigram_list)\n","\n","# Function to calculate bigram probabilities using MLE\n","def calculate_bigram_prob(bigram):\n","    return bigram_counts[bigram] / unigram_counts[bigram[0]]\n","\n","# Test the function with an example bigram\n","example_bigram = ('<START>', 'the')\n","print(\"Probability of\", example_bigram, \":\", calculate_bigram_prob(example_bigram))\n","example_bigram = ('the', 'zebra')\n","print(\"Probability of\", example_bigram, \":\", calculate_bigram_prob(example_bigram))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_sentence_prob(sentence):\n","    sentence_bigram_list = create_bigrams(sentence)\n","    probability = 1\n","    for bigram in sentence_bigram_list:\n","        probability *= calculate_bigram_prob(bigram)\n","    return probability\n","\n","\n","print(\n","    f'Probability of \"why are no-smoking areas not enforced ?\": {calculate_sentence_prob(\"why are no-smoking areas not enforced ?\")}')\n","print(\n","    f'Probability of \"the door is green\": {calculate_sentence_prob(\"the door is green\")}')\n","print(\n","    f'Probability of \"we pass\": {calculate_sentence_prob(\"we pass\")}')"]},{"cell_type":"markdown","metadata":{},"source":["When we encounter a word that did not appear in the training texts, this will result in a probability of zero for any bigram containing this word, making the probability of the entire sentence zero. This is a common issue in language modeling known as the zero-probability problem, and it can be handled using techniques like Laplace (add-one) smoothing.\n","\n","If the sentence is very long, the probability of the sentence will tend to be very small due to the multiplication of probabilities, which can lead to underflow problems in computers. One way to handle this is by working with the log probabilities instead of the raw probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate the vocabulary size\n","vocabulary_size = len(unigram_counts)\n","\n","# Function to calculate bigram probabilities using Laplace smoothing\n","def calculate_bigram_log_prob_with_laplace(bigram):\n","    numerator = bigram_counts[bigram] + 1 \n","    denominator = unigram_counts[bigram[0]] + vocabulary_size\n","    return math.log(numerator) - math.log(denominator)\n","\n","\n","#calculate probability of a sentence\n","def calculate_sentence_prob_improved(sentence):\n","    tokens = ['<START>'] + word_tokenize(sentence.lower())\n","    probability = 0\n","    for i in range(len(tokens) - 1):\n","        bigram = (tokens[i], tokens[i + 1])\n","        probability += calculate_bigram_log_prob_with_laplace(bigram)\n","    return probability\n","\n","\n","print(\n","    f'Log probability of \"why are no-smoking areas not enforced ?\": {calculate_sentence_prob_improved(\"why are no-smoking areas not enforced ?\")}')\n","print(\n","    f'Log probability of \"the door is open\": {calculate_sentence_prob_improved(\"the door is open\")}')\n","print(\n","    f'Log probability of \"the door is green\": {calculate_sentence_prob_improved(\"the door is green\")}')\n","print(\n","    f'Log probability of \"we pass\": {calculate_sentence_prob_improved(\"we pass\")}')\n"]},{"cell_type":"markdown","metadata":{},"source":["The more negative a log probability is, the less likely the sentence is."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Translation modeling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import string\n","\n","def tokenize_corpus(corpus, add_null=False):\n","    \"\"\"Tokenize the input corpus (a list of sentences) into a list of lists of tokens.\n","    Optionally add a NULL token at the beginning of each sentence.\"\"\"\n","    clean_corpus = [sentence.translate(str.maketrans(\n","        '', '', string.punctuation)) for sentence in corpus]\n","    tokenized_corpus = [sentence.lower().split() for sentence in clean_corpus]\n","    if add_null:\n","        for sentence in tokenized_corpus:\n","            sentence.insert(0, \"<NULL>\")\n","    return tokenized_corpus\n","\n","\n","def initialize_translation_prob(corpus_english, corpus_foreign):\n","    \"\"\"Initialize translation probabilities with a lower probability for NULL.\"\"\"\n","\n","    word_correspondence = {}\n","\n","    for sentence_e, sentence_f in zip(corpus_english, corpus_foreign):\n","        for word_e in sentence_e:\n","            if word_e not in word_correspondence:\n","                word_correspondence[word_e] = []\n","            for word_f in sentence_f:\n","                if word_f not in word_correspondence[word_e]:\n","                    word_correspondence[word_e] += [word_f]\n","\n","\n","    translation_prob = {}\n","    null_prob = 0.00001\n","\n","\n","    for word_e in word_correspondence:\n","        for word_f in word_correspondence[word_e]:\n","            if word_f == \"<NULL>\":\n","               translation_prob[(word_e, word_f)] = null_prob\n","            else:\n","                translation_prob[(word_e, word_f)] = (1 - null_prob) / (len(word_correspondence[word_e]) - 1)\n","            #translation_prob[(word_e, word_f)] = 1 / len(word_correspondence[word_e])\n","\n","    return translation_prob\n","\n","\n","\n","\n","print(tokenize_corpus([\"The dog runs\", \"The cat sleeps\"]))\n","print(initialize_translation_prob(tokenize_corpus([\"The dog runs\", \"The cat sleeps\", \"I am\"]), tokenize_corpus([\"Le chien court\", \"Le chat dort\", \"Je suis\"], add_null=True)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","\n","def ibm_model_1(corpus_english, corpus_foreign, iterations=10):\n","    corpus_foreign_tokens = tokenize_corpus(corpus_foreign, add_null=True)  # foreign language corpus\n","    corpus_english_tokens = tokenize_corpus(corpus_english)  # English corpus, with null tokens\n","\n","    # Initialize translation probabilities uniformly\n","    translation_prob = initialize_translation_prob(corpus_english_tokens, corpus_foreign_tokens)\n","\n","    for iteration in range(iterations):\n","        count_ef = defaultdict(float)\n","        total_e = defaultdict(float)\n","        \n","        # E-step: Expectation\n","        for sentence_e, sentence_f in zip(corpus_english_tokens, corpus_foreign_tokens):\n","            for word_f in sentence_f:\n","                s_total_word_e = sum(translation_prob[(word_e, word_f)] for word_e in sentence_e)\n","                for word_e in sentence_e:\n","                    delta = translation_prob[(word_e, word_f)] / s_total_word_e\n","                    \n","                    # Update counts\n","                    count_ef[(word_e, word_f)] += delta\n","                    total_e[word_e] += delta\n","        \n","        # M-step: Maximization\n","        for (word_e, word_f), count in count_ef.items():\n","            translation_prob[(word_e, word_f)] = count / total_e[word_e]\n","\n","\n","        # normalize probabilities\n","        new_dict = {}\n","        for key, value in translation_prob.items():\n","            if key[0] not in new_dict:\n","                new_dict[key[0]] = value\n","            else:\n","                new_dict[key[0]] += value\n","\n","        for key, value in translation_prob.items():\n","            translation_prob[key] = value / new_dict[key[0]]\n","\n","    print(translation_prob)\n","    return translation_prob\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Example usage (using dummy data):\n","corpus1 = [\"the house\", \"the book\", \"a big house\"]\n","corpus2 = [\"das haus\", \"das buch\", \"ein großes haus\"]  # Assuming German for demonstration\n","\n","print(initialize_translation_prob(tokenize_corpus(corpus1), tokenize_corpus(corpus2, add_null=True)))\n","\n","translation_prob = ibm_model_1(corpus1, corpus2, iterations=100)\n","\n","# Find translations for a specific word (e.g., \"house\")\n","translations_for_word = {pair[1]: prob for pair, prob in translation_prob.items() if pair[0] == \"house\"}\n","# Sort translations by probability\n","sorted_translations = sorted(translations_for_word.items(), key=lambda item: item[1], reverse=True)\n","\n","# Print top N translations\n","print(\"Top translations for 'house':\")\n","summm = 0\n","for foreign_word, prob in sorted_translations[:10]:\n","    print(f\"{foreign_word}: {prob}\")\n","    summm += prob\n","print(summm)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["test with swede"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Write code that implements the estimation algorithm for IBM model 1.\n","# Then print, for either Swedish, German, or French, the 10 words that \n","#the English word european is most likely to be translated into, according \n","#to your estimate. It can be interesting to look at this list of 10 words and\n","#see how it changes during the EM iterations.\n","\n","#reduce the size of corpus_de_en and corpus_sv_en\n","sv_en = open(en_sv_file_path, 'r', encoding='utf-8').read()\n","sv_en = sv_en.split(\"\\n\")\n","corpus_en = sv_en[:]\n","\n","sv = open(sv_file_path, 'r', encoding='utf-8').read()\n","sv = sv.split(\"\\n\")\n","corpus_sv = sv[:]\n","\n","# Estimate translation probabilities\n","# translation_prob_sv = ibm_model_1(corpus_en, corpus_sv, iterations=5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#reduce the size of corpus_de_en and corpus_sv_en\n","fr_en = open(en_fr_file_path, 'r', encoding='utf-8').read()\n","fr_en = fr_en.split(\"\\n\")\n","corpus_en = fr_en[:]\n","\n","fr = open(fr_file_path, 'r', encoding='utf-8').read()\n","fr = fr.split(\"\\n\")\n","corpus_fr = fr[:]\n","\n","# Estimate translation probabilities\n","translation_prob_fr = ibm_model_1(\n","    corpus_en, corpus_fr, iterations=3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find translations for a specific word (e.g., \"european\")\n","translations_for_word = {pair[1]: prob for pair,\n","                         prob in translation_prob_fr.items() if pair[0] == \"european\"}\n","# Sort the top 10 translations by probability\n","\n","suma = 0\n","for i in translations_for_word:\n","    suma = suma + translations_for_word[i]\n","french_translations = sorted(\n","    translations_for_word.items(), key=lambda item: item[1], reverse=True)[:10]\n","\n","print(\"Top translations for 'european' in French:\")\n","for foreign_word, prob in french_translations:\n","    print(f\"{foreign_word}: {prob}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["test with french"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Find translations for a specific word (e.g., \"european\")\n","translations_for_word = {pair[1]: prob for pair,\n","                         prob in translation_prob.items() if pair[0] == \"european\"}\n","# Sort the top 10 translations by probability\n","\n","suma = 0\n","for i in translations_for_word:\n","    suma = suma + translations_for_word[i]\n","swedish_translations = sorted(\n","    translations_for_word.items(), key=lambda item: item[1], reverse=True)[:10]\n","\n","print(\"Top translations for 'european' in Swedish:\")\n","for foreign_word, prob in swedish_translations:\n","    print(f\"{foreign_word}: {prob}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Decoding"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import heapq\n","\n","\n","def get_top_n_word_translations(foreign_sentence, translation_prob, n):\n","    word_translations = {}\n","    for word in foreign_sentence:\n","        #get top 5 translations for each word\n","        translations_for_word = {\n","            pair[0]: prob for pair, prob \n","            in translation_prob.items() if pair[1] == word}\n","        \n","        # Sort translations by probability\n","        sorted_translations = sorted(\n","            translations_for_word.items(), \n","            key=lambda item: item[1], reverse=True)[:n]\n","        word_translations[word] = sorted_translations\n","        print(f'word list for {word}: {word_translations[word]}')\n","        print('\\n')\n","    \n","    return word_translations\n","\n","def translate_sentence_approx(sentence, translation_prob, \n","                              n_words=5, beam_width=5):\n","    \"\"\"\n","    Translate one sentence from a foreign language to English.\n","    In the algorithm, we will keep the top n word translations for \n","    each word in the sentence. Moreover, we will use beam search to\n","    keep the most likely translations for the whole sentence.\n","    \"\"\"\n","    \n","    beam = [(0, [])]  # (log_prob, sequence)\n","    top_n_word_translations = get_top_n_word_translations(\n","        sentence, translation_prob, n_words)\n","\n","    # Iterate over each word in the foreign sentence\n","    for word in sentence:\n","        # Get the top translations for the current foreign word\n","        if word in top_n_word_translations:\n","            top_translations = top_n_word_translations[word]\n","        else:\n","            continue\n","\n","        next_beam = []\n","\n","        # Expand each sequence in the beam with \n","        # each translation of the current foreign word\n","        for log_prob, seq in beam:\n","            for (translation, translation_prob) in top_translations:\n","                new_seq = seq + [translation]\n","                \n","                # Update the log probability\n","                new_log_prob = log_prob + math.log(translation_prob)\n","                next_beam.append((new_log_prob, new_seq))\n","                \n","        # Keep only the top `beam_width` sequences\n","        beam = heapq.nlargest(beam_width, next_beam, key=lambda x: x[0])\n","        \n","    prob, highest_prob_sentence = (0, '') if len(beam) == 0 else max(beam, key=lambda x: x[0])\n","    return prob, \" \".join(highest_prob_sentence)\n","\n","\n","french_sentence = \"je suis européenne\".split()\n","prob_translated_sentennce, translated_sentence = translate_sentence_approx(\n","    sentence=french_sentence,  translation_prob=translation_prob_fr, \n","    n_words=5)\n","print(f\"French sentence: {' '.join(french_sentence)}\")\n","print(\n","    f\"Translated sentence: {translated_sentence} (log probability: {prob_translated_sentennce})\")\n"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"a97e5a613ce44d2bbe2564741918b105","deepnote_persisted_session":{"createdAt":"2024-01-29T14:08:51.714Z"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1 | packaged by conda-forge | (main, Dec 23 2023, 08:03:24) [GCC 12.3.0]"},"vscode":{"interpreter":{"hash":"8e21d9f50f7c30e7092d9e7e50ded244397ed7adf91fc30361913d3d8a47365c"}}},"nbformat":4,"nbformat_minor":0}
