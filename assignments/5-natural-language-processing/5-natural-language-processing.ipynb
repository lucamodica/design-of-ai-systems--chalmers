{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a429067d13d64b3c91f285ed1aedc5eb","deepnote_cell_type":"markdown"},"source":["# Assignment 5 - Natural Language Processing\n","\n","- Student 1 - Luca Modica\n","- Student 2 - Hugo Alves Henriques E Silva\n","\n","---"]},{"cell_type":"code","execution_count":472,"metadata":{"cell_id":"a0b8ec62945f45d490d3dcc6efc9f49e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":852,"execution_start":1706548661380,"source_hash":"820f4cc4"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import math\n","\n","sns.set_style()\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"cell_id":"c75378b090bf4bb192cc08d6d1c5999c","deepnote_cell_type":"markdown"},"source":["## Reading data"]},{"cell_type":"code","execution_count":473,"metadata":{},"outputs":[],"source":["from collections import Counter\n","import re\n","\n","# Paths to the files\n","de_file_path = 'dat410_europarl/europarl-v7.de-en.lc.de'\n","en_de_file_path = 'dat410_europarl/europarl-v7.de-en.lc.en'\n","fr_file_path = 'dat410_europarl/europarl-v7.fr-en.lc.fr'\n","en_fr_file_path = 'dat410_europarl/europarl-v7.fr-en.lc.en'\n","sv_file_path = 'dat410_europarl/europarl-v7.sv-en.lc.sv'\n","en_sv_file_path = 'dat410_europarl/europarl-v7.sv-en.lc.en'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Warmup"]},{"cell_type":"code","execution_count":474,"metadata":{},"outputs":[],"source":["# Function to read a file and return word frequencies\n","def get_word_frequencies(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read().lower()  # Ensure all text is lowercase\n","        words = re.findall(r'\\b\\w+\\b', text)  # Extract words\n","        word_freq = Counter(words)  # Count word frequencies\n","    return word_freq\n"]},{"cell_type":"code","execution_count":475,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Most common words in German: [('die', 10521), ('der', 9374), ('und', 7028), ('in', 4175), ('zu', 3169), ('den', 2976), ('wir', 2863), ('daß', 2738), ('ich', 2670), ('das', 2669)]\n","Most common words in English (German-English pair): [('the', 19853), ('of', 9633), ('to', 9069), ('and', 7307), ('in', 6278), ('is', 4478), ('that', 4441), ('a', 4438), ('we', 3372), ('this', 3362)]\n","Most common words in French: [('apos', 16729), ('de', 14528), ('la', 9746), ('et', 6620), ('l', 6536), ('le', 6177), ('à', 5588), ('les', 5587), ('des', 5232), ('que', 4797)]\n","Most common words in English (French-English pair): [('the', 19627), ('of', 9534), ('to', 8992), ('and', 7214), ('in', 6197), ('is', 4453), ('that', 4421), ('a', 4388), ('we', 3341), ('this', 3332)]\n","Most common words in Swedish: [('att', 9181), ('och', 7038), ('i', 5954), ('det', 5687), ('som', 5028), ('för', 4959), ('av', 4013), ('är', 3840), ('en', 3724), ('vi', 3211)]\n","Most common words in English (Swedish-English pair): [('the', 19327), ('of', 9344), ('to', 8814), ('and', 6949), ('in', 6124), ('is', 4400), ('that', 4357), ('a', 4271), ('we', 3223), ('this', 3222)]\n"]}],"source":["# Get word frequencies for German-English pair\n","de_word_freq = get_word_frequencies(de_file_path)\n","en_de_word_freq = get_word_frequencies(en_de_file_path)\n","\n","# Print the 10 most common words in German and English (German-English pair)\n","de_common_words = de_word_freq.most_common(10)\n","en_de_common_words = en_de_word_freq.most_common(10)\n","\n","# Get word frequencies for French-English pair\n","fr_word_freq = get_word_frequencies(fr_file_path)\n","en_fr_word_freq = get_word_frequencies(en_fr_file_path)\n","\n","# Get word frequencies for Swedish-English pair\n","sv_word_freq = get_word_frequencies(sv_file_path)\n","en_sv_word_freq = get_word_frequencies(en_sv_file_path)\n","\n","# Print the 10 most common words in French, English (French-English pair), Swedish, and English (Swedish-English pair)\n","fr_common_words = fr_word_freq.most_common(10)\n","en_fr_common_words = en_fr_word_freq.most_common(10)\n","sv_common_words = sv_word_freq.most_common(10)\n","en_sv_common_words = en_sv_word_freq.most_common(10)\n","\n","print(\"Most common words in German:\", de_common_words)\n","print(\"Most common words in English (German-English pair):\", en_de_common_words)\n","print(\"Most common words in French:\", fr_common_words)\n","print(\"Most common words in English (French-English pair):\", en_fr_common_words)\n","print(\"Most common words in Swedish:\", sv_common_words)\n","print(\"Most common words in English (Swedish-English pair):\", en_sv_common_words)\n","\n"]},{"cell_type":"code","execution_count":476,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words: 784458\n","Speaker count: 33\n","Zebra count: 0\n","Probability of 'speaker': 4.206726172720528e-05\n","Probability of 'zebra': 0.0\n"]}],"source":["# Calculate the total word counts and the counts for 'speaker' and 'zebra' across all English files\n","total_words = sum(en_de_word_freq.values()) + sum(en_fr_word_freq.values()) + sum(en_sv_word_freq.values())\n","speaker_count = en_de_word_freq['speaker'] + en_fr_word_freq['speaker'] + en_sv_word_freq['speaker']\n","zebra_count = en_de_word_freq['zebra'] + en_fr_word_freq['zebra'] + en_sv_word_freq['zebra']\n","\n","# Calculate probabilities\n","prob_speaker = speaker_count / total_words\n","prob_zebra = zebra_count / total_words\n","\n","print(\"Total words:\", total_words)\n","print(\"Speaker count:\", speaker_count)\n","print(\"Zebra count:\", zebra_count)\n","print(\"Probability of 'speaker':\", prob_speaker)\n","print(\"Probability of 'zebra':\", prob_zebra)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Language modeling"]},{"cell_type":"code","execution_count":477,"metadata":{},"outputs":[{"data":{"text/plain":["(0.11425524114255241, 9.355332539547542e-05)"]},"execution_count":477,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.tokenize import word_tokenize\n","from nltk.util import bigrams\n","\n","# Function to tokenize corpus into bigrams with start and end tokens\n","def create_bigrams(text):\n","    sentences = text.split('\\n')\n","    bigram_list = []\n","    for sentence in sentences:\n","        tokens = ['<START>'] + word_tokenize(sentence) + ['<END>']\n","        bigram_list.extend(list(bigrams(tokens)))\n","    return bigram_list\n","\n","# Read the English text files from all three pairs to create a single corpus\n","corpus_de_en = open(en_de_file_path, 'r', encoding='utf-8').read()\n","corpus_fr_en = open(en_fr_file_path, 'r', encoding='utf-8').read()\n","corpus_sv_en = open(en_sv_file_path, 'r', encoding='utf-8').read()\n","\n","# Combine the corpora\n","combined_corpus = '\\n'.join([corpus_de_en, corpus_fr_en, corpus_sv_en])\n","\n","# Create bigrams from the combined corpus\n","bigram_list = create_bigrams(combined_corpus)\n","\n","# Calculate bigram and unigram counts\n","unigram_counts = Counter([unigram for bigram in bigram_list for unigram in bigram])\n","bigram_counts = Counter(bigram_list)\n","\n","# Function to calculate bigram probabilities using MLE\n","def calculate_bigram_probability(bigram):\n","    return bigram_counts[bigram] / unigram_counts[bigram[0]]\n","\n","# Test the function with an example bigram\n","calculate_bigram_probability(('<START>', 'the')), calculate_bigram_probability(('the', 'door'))\n"]},{"cell_type":"markdown","metadata":{},"source":["When we encounter a word that did not appear in the training texts, this will result in a probability of zero for any bigram containing this word, making the probability of the entire sentence zero. This is a common issue in language modeling known as the zero-probability problem, and it can be handled using techniques like Laplace (add-one) smoothing.\n","\n","If the sentence is very long, the probability of the sentence will tend to be very small due to the multiplication of probabilities, which can lead to underflow problems in computers. One way to handle this is by working with the log probabilities instead of the raw probabilities."]},{"cell_type":"code","execution_count":478,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-2.503971351992991 -9.28666077584931\n","-39.20473330301283\n","-41.13676695947791\n","-29.65130549737112\n"]}],"source":["# Calculate the vocabulary size\n","vocabulary_size = len(unigram_counts)\n","\n","# Function to calculate bigram probabilities using Laplace smoothing\n","def calculate_bigram_log_probability_with_laplace(bigram):\n","    numerator = bigram_counts[bigram] + 1  # Add one to the count for Laplace smoothing\n","    denominator = unigram_counts[bigram[0]] + vocabulary_size  # Add vocabulary size for Laplace smoothing\n","    return math.log(numerator) - math.log(denominator)\n","\n","\n","print(calculate_bigram_log_probability_with_laplace(('<START>', 'the')), calculate_bigram_log_probability_with_laplace(('the', 'door')))\n","\n","#calculate probability of a sentence\n","def calculate_sentence_probability(sentence):\n","    tokens = ['<START>'] + word_tokenize(sentence.lower()) + ['<END>']\n","    probability = 0\n","    for i in range(len(tokens) - 1):\n","        bigram = (tokens[i], tokens[i + 1])\n","        probability += calculate_bigram_log_probability_with_laplace(bigram)\n","    return probability\n","\n","# Test the function with an example sentence\n","print(calculate_sentence_probability('the door is open'))\n","print(calculate_sentence_probability('the door is green'))\n","print(calculate_sentence_probability(\"The number of people\"))"]},{"cell_type":"markdown","metadata":{},"source":["The more negative a log probability is, the less likely the sentence is."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Translation modeling"]},{"cell_type":"code","execution_count":491,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['the', 'dog', 'runs'], ['the', 'cat', 'sleeps']]\n","[['null', 'le', 'chien', 'court'], ['null', 'le', 'chat', 'dort'], ['null', 'je', 'suis']]\n","{('the', 'null'): 0.001, ('the', 'le'): 0.1998, ('the', 'chien'): 0.1998, ('the', 'court'): 0.1998, ('the', 'chat'): 0.1998, ('the', 'dort'): 0.1998, ('dog', 'null'): 0.001, ('dog', 'le'): 0.333, ('dog', 'chien'): 0.333, ('dog', 'court'): 0.333, ('runs', 'null'): 0.001, ('runs', 'le'): 0.333, ('runs', 'chien'): 0.333, ('runs', 'court'): 0.333, ('cat', 'null'): 0.001, ('cat', 'le'): 0.333, ('cat', 'chat'): 0.333, ('cat', 'dort'): 0.333, ('sleeps', 'null'): 0.001, ('sleeps', 'le'): 0.333, ('sleeps', 'chat'): 0.333, ('sleeps', 'dort'): 0.333, ('i', 'null'): 0.001, ('i', 'je'): 0.4995, ('i', 'suis'): 0.4995, ('am', 'null'): 0.001, ('am', 'je'): 0.4995, ('am', 'suis'): 0.4995}\n"]}],"source":["def tokenize_corpus(corpus, add_null=False):\n","    \"\"\"Tokenize the input corpus (a list of sentences) into a list of lists of tokens.\n","    Optionally add a NULL token at the beginning of each sentence.\"\"\"\n","    tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n","    if add_null:\n","        for sentence in tokenized_corpus:\n","            sentence.insert(0, \"null\")\n","    return tokenized_corpus\n","\n","\n","def initialize_translation_prob(corpus_english, corpus_foreign, add_null=False):\n","    \"\"\"Initialize translation probabilities with a lower probability for NULL.\"\"\"\n","\n","    word_correspondence = {}\n","\n","    if add_null:\n","        for sentence_f in corpus_foreign:\n","            sentence_f.insert(0, \"null\")\n","\n","    for sentence_e, sentence_f in zip(corpus_english, corpus_foreign):\n","        for word_e in sentence_e:\n","            if word_e not in word_correspondence:\n","                word_correspondence[word_e] = []\n","            for word_f in sentence_f:\n","                if word_f not in word_correspondence[word_e]:\n","                    word_correspondence[word_e] += [word_f]\n","\n","\n","    translation_prob = {}\n","    null_prob = 0.001\n","\n","    print(corpus_foreign)\n","\n","\n","    for word_e in word_correspondence:\n","        for word_f in word_correspondence[word_e]:\n","            if word_f == \"null\":\n","               translation_prob[(word_e, word_f)] = null_prob\n","            else:\n","                translation_prob[(word_e, word_f)] = (1 - null_prob) / (len(word_correspondence[word_e]) - 1)\n","            #translation_prob[(word_e, word_f)] = 1 / len(word_correspondence[word_e])\n","\n","    return translation_prob\n","\n","\n","\n","\n","print(tokenize_corpus([\"The dog runs\", \"The cat sleeps\"]))\n","print(initialize_translation_prob(tokenize_corpus([\"The dog runs\", \"The cat sleeps\", \"I am\"]), tokenize_corpus([\"Le chien court\", \"Le chat dort\", \"Je suis\"]), add_null=True))"]},{"cell_type":"code","execution_count":480,"metadata":{},"outputs":[{"data":{"text/plain":["' def ibm_model_1(corpus_english, corpus_foreign, iterations=10):\\n    corpus1_tokens = tokenize_corpus(corpus_english)\\n    corpus2_tokens = tokenize_corpus(corpus_foreign, add_null=True)\\n    \\n    # Initialize translation probabilities uniformly\\n    translation_prob = initialize_translation_prob(corpus1_tokens, corpus2_tokens)\\n\\n    #print(translation_prob)\\n    \\n    for iteration in range(iterations):\\n        # Initialize counts\\n        count = {}\\n        total = {}\\n        for word1 in translation_prob:\\n            total[word1[0]] = 0\\n            for word2 in translation_prob:\\n                count[(word1[0], word2[1])] = 0\\n        \\n        # E-step: Collect counts\\n        for sentence1, sentence2 in zip(corpus1_tokens, corpus2_tokens):\\n            # Compute normalization\\n            #compute alignment probability for each word in sentence1\\n            for word1 in sentence1:\\n                total[word1] = sum(translation_prob[(word1, word2)] for word2 in sentence2)\\n            \\n            # update pseudo-counts using the normalized alignment probabilities\\n            for word1 in sentence1:\\n                for word2 in sentence2:\\n                    word_translation_prob = translation_prob[(word1, word2)]\\n                    count[(word1, word2)] += word_translation_prob / total[word1]\\n                    total[word1] += word_translation_prob / total[word1]   #pseudo-code does not include this part\\n        \\n        # M-step: Estimate probabilities\\n        for word1 in translation_prob:\\n            for word2 in translation_prob:\\n                if total[word1[0]] > 0:  # Avoid division by zero\\n                    translation_prob[(word1[0], word2[1])] = count[(word1[0], word2[1])] / total[word1[0]]\\n    \\n    return translation_prob\\n '"]},"execution_count":480,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\" def ibm_model_1(corpus_english, corpus_foreign, iterations=10):\n","    corpus1_tokens = tokenize_corpus(corpus_english)\n","    corpus2_tokens = tokenize_corpus(corpus_foreign, add_null=True)\n","    \n","    # Initialize translation probabilities uniformly\n","    translation_prob = initialize_translation_prob(corpus1_tokens, corpus2_tokens)\n","\n","    #print(translation_prob)\n","    \n","    for iteration in range(iterations):\n","        # Initialize counts\n","        count = {}\n","        total = {}\n","        for word1 in translation_prob:\n","            total[word1[0]] = 0\n","            for word2 in translation_prob:\n","                count[(word1[0], word2[1])] = 0\n","        \n","        # E-step: Collect counts\n","        for sentence1, sentence2 in zip(corpus1_tokens, corpus2_tokens):\n","            # Compute normalization\n","            #compute alignment probability for each word in sentence1\n","            for word1 in sentence1:\n","                total[word1] = sum(translation_prob[(word1, word2)] for word2 in sentence2)\n","            \n","            # update pseudo-counts using the normalized alignment probabilities\n","            for word1 in sentence1:\n","                for word2 in sentence2:\n","                    word_translation_prob = translation_prob[(word1, word2)]\n","                    count[(word1, word2)] += word_translation_prob / total[word1]\n","                    total[word1] += word_translation_prob / total[word1]   #pseudo-code does not include this part\n","        \n","        # M-step: Estimate probabilities\n","        for word1 in translation_prob:\n","            for word2 in translation_prob:\n","                if total[word1[0]] > 0:  # Avoid division by zero\n","                    translation_prob[(word1[0], word2[1])] = count[(word1[0], word2[1])] / total[word1[0]]\n","    \n","    return translation_prob\n"," \"\"\""]},{"cell_type":"code","execution_count":496,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","\n","def ibm_model_1(corpus_english, corpus_foreign, iterations=10):\n","    # Assuming tokenize_corpus adds a \"null\" token to the beginning of each English sentence\n","    # and splits sentences into lists of words.\n","    corpus_foreign_tokens = tokenize_corpus(corpus_foreign, add_null=True)  # foreign language corpus\n","    corpus_english_tokens = tokenize_corpus(corpus_english)  # English corpus, with null word\n","\n","    # Initialize translation probabilities uniformly\n","    translation_prob = initialize_translation_prob(corpus_english_tokens, corpus_foreign_tokens)\n","\n","    for iteration in range(iterations):\n","        count_ef = defaultdict(float)\n","        total_f = defaultdict(float)\n","        \n","        # E-step: Expectation\n","        for sentence_e, sentence_f in zip(corpus_english_tokens, corpus_foreign_tokens):\n","            # For each word in the english sentence\n","            for word_e in sentence_e:\n","                # Compute normalization factor for the word2\n","                s_total_word_f = sum(translation_prob[(word_e, word_f)] for word_f in sentence_f)\n","                # For each word in the foreign sentence\n","                for word_f in sentence_f:\n","                    # Calculate delta, which is the proportion of the alignment probability of the word2 to the word1\n","                    delta = translation_prob[(word_e, word_f)] / s_total_word_f\n","                    # Update counts\n","                    count_ef[(word_e, word_f)] += delta\n","                    total_f[word_f] += delta\n","        \n","        # M-step: Maximization\n","        for (word_e, word_f), count in count_ef.items():\n","            translation_prob[(word_e, word_f)] = count / total_f[word_f]\n","\n","\n","        new_dict = {}\n","        for key, value in translation_prob.items():\n","            if key[0] not in new_dict:\n","                new_dict[key[0]] = value\n","            else:\n","                new_dict[key[0]] += value\n","\n","        for key, value in translation_prob.items():\n","            translation_prob[key] = value / new_dict[key[0]]\n","\n","\n","            # house - null = 0.001\n","\n","    return translation_prob\n","\n"]},{"cell_type":"code","execution_count":504,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['null', 'das', 'haus'], ['null', 'das', 'buch'], ['null', 'ein', 'großes', 'haus']]\n","{('the', 'null'): 0.001, ('the', 'das'): 0.333, ('the', 'haus'): 0.333, ('the', 'buch'): 0.333, ('house', 'null'): 0.001, ('house', 'das'): 0.24975, ('house', 'haus'): 0.24975, ('house', 'ein'): 0.24975, ('house', 'großes'): 0.24975, ('book', 'null'): 0.001, ('book', 'das'): 0.4995, ('book', 'buch'): 0.4995, ('a', 'null'): 0.001, ('a', 'ein'): 0.333, ('a', 'großes'): 0.333, ('a', 'haus'): 0.333, ('big', 'null'): 0.001, ('big', 'ein'): 0.333, ('big', 'großes'): 0.333, ('big', 'haus'): 0.333}\n","[['null', 'das', 'haus'], ['null', 'das', 'buch'], ['null', 'ein', 'großes', 'haus']]\n","Top translations for 'house':\n","haus: 0.6666666666666206\n","null: 0.33333333333337944\n","ein: 1.3980678439694192e-18\n","großes: 1.3980678439694192e-18\n","das: 8.584276825010788e-31\n","1.0\n"]}],"source":["# Example usage (using dummy data):\n","corpus1 = [\"the house\", \"the book\", \"a big house\"]\n","corpus2 = [\"das haus\", \"das buch\", \"ein großes haus\"]  # Assuming German for demonstration\n","\n","print(initialize_translation_prob(tokenize_corpus(corpus1), tokenize_corpus(corpus2, add_null=True)))\n","\n","# Estimate translation probabilities\n","translation_prob = ibm_model_1(corpus1, corpus2, iterations=100)\n","\n","# Find translations for a specific word (e.g., \"house\")\n","translations_for_word = {pair[1]: prob for pair, prob in translation_prob.items() if pair[0] == \"house\"}\n","# Sort translations by probability\n","sorted_translations = sorted(translations_for_word.items(), key=lambda item: item[1], reverse=True)\n","\n","# Print top N translations\n","print(\"Top translations for 'house':\")\n","summm = 0\n","for foreign_word, prob in sorted_translations[:10]:\n","    print(f\"{foreign_word}: {prob}\")\n","    summm += prob\n","print(summm)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Need to be very careful with the NULL probability. It needs to be way lower than the other probabilities, otherwise the model will always choose the NULL translation, because NULL will be present in every sentence. Increasing the number of iterations will eventually make null the most probable translation for every word."]},{"cell_type":"code","execution_count":514,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unexpected exception formatting exception. Falling back to standard exception\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/var/folders/0v/j168s2s91bxf0vr_r7rn3m_c0000gn/T/ipykernel_7046/3252658687.py\", line 22, in <module>\n","    translation_prob = ibm_model_1(corpus_en, corpus_sv, iterations=5)\n","                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/var/folders/0v/j168s2s91bxf0vr_r7rn3m_c0000gn/T/ipykernel_7046/2735951893.py\", line 10, in ibm_model_1\n","    translation_prob = initialize_translation_prob(corpus_english_tokens, corpus_foreign_tokens)\n","                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/var/folders/0v/j168s2s91bxf0vr_r7rn3m_c0000gn/T/ipykernel_7046/3091769212.py\", line -1, in initialize_translation_prob\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(\n","          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1428, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1319, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1172, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1087, in format_exception_as_a_whole\n","    frames.append(self.format_record(record))\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 969, in format_record\n","    frame_info.lines, Colors, self.has_colors, lvals\n","    ^^^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n","    return self._sd.lines\n","           ^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n","    pieces = self.included_pieces\n","             ^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n","    pos = scope_pieces.index(self.executing_piece)\n","                             ^^^^^^^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n","    value = obj.__dict__[self.func.__name__] = self.func(obj)\n","                                               ^^^^^^^^^^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n","    return only(\n","           ^^^^^\n","  File \"/Users/hugo/anaconda3/envs/hugo-work/lib/python3.11/site-packages/executing/executing.py\", line 190, in only\n","    raise NotOneValueFound('Expected one value, found 0')\n","executing.executing.NotOneValueFound: Expected one value, found 0\n"]}],"source":["#Write code that implements the estimation algorithm for IBM model 1.\n","# Then print, for either Swedish, German, or French, the 10 words that \n","#the English word european is most likely to be translated into, according \n","#to your estimate. It can be interesting to look at this list of 10 words and\n","#see how it changes during the EM iterations.\n","\n","#reduce the size of corpus_de_en and corpus_sv_en\n","sv_en = open(en_sv_file_path, 'r', encoding='utf-8').read()\n","#separate the sentences\n","sv_en = sv_en.split(\"\\n\")\n","corpus_en = sv_en[:]\n","\n","sv = open(sv_file_path, 'r', encoding='utf-8').read()\n","#separate the sentences\n","sv = sv.split(\"\\n\")\n","corpus_sv = sv[:]\n","\n","#print(corpus_en)\n","#print(corpus_sv)\n","\n","# Estimate translation probabilities\n","translation_prob = ibm_model_1(corpus_en, corpus_sv, iterations=5)\n","\n","# Find translations for a specific word (e.g., \"european\")\n","translations_for_word = {pair[1]: prob for pair, prob in translation_prob.items() if pair[0] == \"european\"}\n","# Sort the top 10 translations by probability\n","\n","suma = 0\n","\n","for i in translations_for_word:\n","    suma = suma + translations_for_word[i]\n","swedish_translations = sorted(translations_for_word.items(), key=lambda item: item[1], reverse=True)[:10]"]},{"cell_type":"code","execution_count":513,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top translations for 'european' in Swedish:\n","europeisk: 0.031210200710030948\n","europeiska: 0.028526566806046767\n","europeiskt: 0.02606960952098505\n","europaparlamentet: 0.01719412108788691\n","europaparlamentets: 0.014530652419929827\n","bitter: 0.012004112427672357\n","lugnas: 0.011629937748564695\n","valdeltagandet: 0.009103269515693112\n","europaval: 0.008833475982950558\n","sa: 0.008655547697343925\n","0.9999999999999987\n"]}],"source":["print(\"Top translations for 'european' in Swedish:\")\n","\n","for foreign_word, prob in swedish_translations:\n","    print(f\"{foreign_word}: {prob}\")\n","\n","print(suma)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Decoding"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"a97e5a613ce44d2bbe2564741918b105","deepnote_persisted_session":{"createdAt":"2024-01-29T14:08:51.714Z"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
