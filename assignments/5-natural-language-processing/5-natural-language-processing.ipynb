{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"a429067d13d64b3c91f285ed1aedc5eb","deepnote_cell_type":"markdown"},"source":["# Assignment 5 - Natural Language Processing\n","\n","- Student 1 - Luca Modica\n","- Student 2 - Hugo Alves Henriques E Silva\n","\n","---"]},{"cell_type":"code","execution_count":146,"metadata":{"cell_id":"a0b8ec62945f45d490d3dcc6efc9f49e","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":852,"execution_start":1706548661380,"source_hash":"820f4cc4"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n","import math\n","\n","sns.set_style()\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"cell_id":"c75378b090bf4bb192cc08d6d1c5999c","deepnote_cell_type":"markdown"},"source":["## Reading data"]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[],"source":["from collections import Counter\n","import re\n","\n","# Paths to the files\n","de_file_path = 'dat410_europarl/europarl-v7.de-en.lc.de'\n","en_de_file_path = 'dat410_europarl/europarl-v7.de-en.lc.en'\n","fr_file_path = 'dat410_europarl/europarl-v7.fr-en.lc.fr'\n","en_fr_file_path = 'dat410_europarl/europarl-v7.fr-en.lc.en'\n","sv_file_path = 'dat410_europarl/europarl-v7.sv-en.lc.sv'\n","en_sv_file_path = 'dat410_europarl/europarl-v7.sv-en.lc.en'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Warmup"]},{"cell_type":"code","execution_count":148,"metadata":{},"outputs":[],"source":["# Function to read a file and return word frequencies\n","def get_word_frequencies(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        text = file.read().lower()  # Ensure all text is lowercase\n","        words = re.findall(r'\\b\\w+\\b', text)  # Extract words\n","        word_freq = Counter(words)  # Count word frequencies\n","    return word_freq\n"]},{"cell_type":"code","execution_count":149,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Most common words in German:\n","die (10521 occurrences), der (9374 occurrences), und (7028 occurrences), in (4175 occurrences), zu (3169 occurrences), den (2976 occurrences), wir (2863 occurrences), daß (2738 occurrences), ich (2670 occurrences), das (2669 occurrences)\n","Most common words in English (German-English pair):\n","the (19853 occurrences), of (9633 occurrences), to (9069 occurrences), and (7307 occurrences), in (6278 occurrences), is (4478 occurrences), that (4441 occurrences), a (4438 occurrences), we (3372 occurrences), this (3362 occurrences)\n","Most common words in French:\n","apos (16729 occurrences), de (14528 occurrences), la (9746 occurrences), et (6620 occurrences), l (6536 occurrences), le (6177 occurrences), à (5588 occurrences), les (5587 occurrences), des (5232 occurrences), que (4797 occurrences)\n","Most common words in English (French-English pair):\n","the (19627 occurrences), of (9534 occurrences), to (8992 occurrences), and (7214 occurrences), in (6197 occurrences), is (4453 occurrences), that (4421 occurrences), a (4388 occurrences), we (3341 occurrences), this (3332 occurrences)\n","Most common words in Swedish:\n","att (9181 occurrences), och (7038 occurrences), i (5954 occurrences), det (5687 occurrences), som (5028 occurrences), för (4959 occurrences), av (4013 occurrences), är (3840 occurrences), en (3724 occurrences), vi (3211 occurrences)\n","Most common words in English (Swedish-English pair):\n","the (19327 occurrences), of (9344 occurrences), to (8814 occurrences), and (6949 occurrences), in (6124 occurrences), is (4400 occurrences), that (4357 occurrences), a (4271 occurrences), we (3223 occurrences), this (3222 occurrences)\n"]}],"source":["# Get word frequencies for German-English pair\n","de_word_freq = get_word_frequencies(de_file_path)\n","en_de_word_freq = get_word_frequencies(en_de_file_path)\n","\n","# Print the 10 most common words in German and English (German-English pair)\n","de_common_words = de_word_freq.most_common(10)\n","en_de_common_words = en_de_word_freq.most_common(10)\n","\n","# Get word frequencies for French-English pair\n","fr_word_freq = get_word_frequencies(fr_file_path)\n","en_fr_word_freq = get_word_frequencies(en_fr_file_path)\n","\n","# Get word frequencies for Swedish-English pair\n","sv_word_freq = get_word_frequencies(sv_file_path)\n","en_sv_word_freq = get_word_frequencies(en_sv_file_path)\n","\n","# Print the 10 most common words in French, English (French-English pair), Swedish, and English (Swedish-English pair)\n","fr_common_words = fr_word_freq.most_common(10)\n","en_fr_common_words = en_fr_word_freq.most_common(10)\n","sv_common_words = sv_word_freq.most_common(10)\n","en_sv_common_words = en_sv_word_freq.most_common(10)\n","\n","print(\"Most common words in German:\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in de_common_words]))\n","\n","print(\"Most common words in English (German-English pair):\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in en_de_common_words]))\n","\n","print(\"Most common words in French:\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in fr_common_words]))\n","\n","print(\"Most common words in English (French-English pair):\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in en_fr_common_words]))\n","\n","print(\"Most common words in Swedish:\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in sv_common_words]))\n","\n","print(\"Most common words in English (Swedish-English pair):\")\n","print(\", \".join(\n","    [f\"{word} ({count} occurrences)\" for word, count in en_sv_common_words]))\n"]},{"cell_type":"code","execution_count":150,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words: 784458\n","Speaker count: 33\n","Zebra count: 0\n","Probability of 'speaker': 4.206726172720528e-05\n","Probability of 'zebra': 0.0\n"]}],"source":["# Calculate the total word counts and the counts for 'speaker' and 'zebra' across all English files\n","total_words = sum(en_de_word_freq.values()) + sum(en_fr_word_freq.values()) + sum(en_sv_word_freq.values())\n","speaker_count = en_de_word_freq['speaker'] + en_fr_word_freq['speaker'] + en_sv_word_freq['speaker']\n","zebra_count = en_de_word_freq['zebra'] + en_fr_word_freq['zebra'] + en_sv_word_freq['zebra']\n","\n","# Calculate probabilities\n","prob_speaker = speaker_count / total_words\n","prob_zebra = zebra_count / total_words\n","\n","print(\"Total words:\", total_words)\n","print(\"Speaker count:\", speaker_count)\n","print(\"Zebra count:\", zebra_count)\n","print(\"Probability of 'speaker':\", prob_speaker)\n","print(\"Probability of 'zebra':\", prob_zebra)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Language modeling"]},{"cell_type":"code","execution_count":151,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[151], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m# Calculate bigram and unigram counts\u001b[39;00m\n\u001b[1;32m     25\u001b[0m unigram_counts \u001b[39m=\u001b[39m Counter([unigram \u001b[39mfor\u001b[39;00m bigram \u001b[39min\u001b[39;00m bigram_list \u001b[39mfor\u001b[39;00m unigram \u001b[39min\u001b[39;00m bigram])\n\u001b[0;32m---> 26\u001b[0m bigram_counts \u001b[39m=\u001b[39m Counter(bigram_list)\n\u001b[1;32m     28\u001b[0m \u001b[39m# Function to calculate bigram probabilities using MLE\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_bigram_prob\u001b[39m(bigram):\n","File \u001b[0;32m~/mambaforge/envs/design-ai/lib/python3.12/collections/__init__.py:607\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[39mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[39mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m \n\u001b[1;32m    605\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 607\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(iterable, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n","File \u001b[0;32m~/mambaforge/envs/design-ai/lib/python3.12/collections/__init__.py:698\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mupdate(iterable)\n\u001b[1;32m    697\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 698\u001b[0m         _count_elements(\u001b[39mself\u001b[39;49m, iterable)\n\u001b[1;32m    699\u001b[0m \u001b[39mif\u001b[39;00m kwds:\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(kwds)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from nltk.tokenize import word_tokenize\n","from nltk.util import bigrams\n","\n","# Function to tokenize corpus into bigrams with start and end tokens\n","def create_bigrams(text):\n","    sentences = text.split('\\n')\n","    bigram_list = []\n","    for sentence in sentences:\n","        tokens = ['<START>'] + word_tokenize(sentence)\n","        bigram_list.extend(list(bigrams(tokens)))\n","    return bigram_list\n","\n","# Read the English text files from all three pairs to create a single corpus\n","corpus_de_en = open(en_de_file_path, 'r', encoding='utf-8').read()\n","corpus_fr_en = open(en_fr_file_path, 'r', encoding='utf-8').read()\n","corpus_sv_en = open(en_sv_file_path, 'r', encoding='utf-8').read()\n","\n","# Combine the corpora\n","combined_corpus = '\\n'.join([corpus_de_en, corpus_fr_en, corpus_sv_en])\n","\n","# Create bigrams from the combined corpus\n","bigram_list = create_bigrams(combined_corpus)\n","\n","# Calculate bigram and unigram counts\n","unigram_counts = Counter([unigram for bigram in bigram_list for unigram in bigram])\n","bigram_counts = Counter(bigram_list)\n","\n","# Function to calculate bigram probabilities using MLE\n","def calculate_bigram_prob(bigram):\n","    return bigram_counts[bigram] / unigram_counts[bigram[0]]\n","\n","# Test the function with an example bigram\n","example_bigram = ('<START>', 'the')\n","print(\"Probability of\", example_bigram, \":\", calculate_bigram_prob(example_bigram))\n","example_bigram = ('the', 'zebra')\n","print(\"Probability of\", example_bigram, \":\", calculate_bigram_prob(example_bigram))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Probability of \"why are no-smoking areas not enforced ?\": 7.157743716417319e-18\n","Probability of \"the door is green\": 0.0\n","Probability of \"we pass\": 2.0531400966183576e-05\n"]}],"source":["def calculate_sentence_prob(sentence):\n","    sentence_bigram_list = create_bigrams(sentence)\n","    probability = 1\n","    for bigram in sentence_bigram_list:\n","        probability *= calculate_bigram_prob(bigram)\n","    return probability\n","\n","\n","print(\n","    f'Probability of \"why are no-smoking areas not enforced ?\": {calculate_sentence_prob(\"why are no-smoking areas not enforced ?\")}')\n","print(\n","    f'Probability of \"the door is green\": {calculate_sentence_prob(\"the door is green\")}')\n","print(\n","    f'Probability of \"we pass\": {calculate_sentence_prob(\"we pass\")}')"]},{"cell_type":"markdown","metadata":{},"source":["When we encounter a word that did not appear in the training texts, this will result in a probability of zero for any bigram containing this word, making the probability of the entire sentence zero. This is a common issue in language modeling known as the zero-probability problem, and it can be handled using techniques like Laplace (add-one) smoothing.\n","\n","If the sentence is very long, the probability of the sentence will tend to be very small due to the multiplication of probabilities, which can lead to underflow problems in computers. One way to handle this is by working with the log probabilities instead of the raw probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Log probability of \"why are no-smoking areas not enforced ?\": -57.44415862256026\n","Log probability of \"the door is open\": -29.796067530994655\n","Log probability of \"the door is green\": -31.741977680049967\n","Log probability of \"we pass\": -11.416178508839266\n"]}],"source":["# Calculate the vocabulary size\n","vocabulary_size = len(unigram_counts)\n","\n","# Function to calculate bigram probabilities using Laplace smoothing\n","def calculate_bigram_log_prob_with_laplace(bigram):\n","    numerator = bigram_counts[bigram] + 1 \n","    denominator = unigram_counts[bigram[0]] + vocabulary_size\n","    return math.log(numerator) - math.log(denominator)\n","\n","\n","#calculate probability of a sentence\n","def calculate_sentence_prob_improved(sentence):\n","    tokens = ['<START>'] + word_tokenize(sentence.lower())\n","    probability = 0\n","    for i in range(len(tokens) - 1):\n","        bigram = (tokens[i], tokens[i + 1])\n","        probability += calculate_bigram_log_prob_with_laplace(bigram)\n","    return probability\n","\n","\n","print(\n","    f'Log probability of \"why are no-smoking areas not enforced ?\": {calculate_sentence_prob_improved(\"why are no-smoking areas not enforced ?\")}')\n","print(\n","    f'Log probability of \"the door is open\": {calculate_sentence_prob_improved(\"the door is open\")}')\n","print(\n","    f'Log probability of \"the door is green\": {calculate_sentence_prob_improved(\"the door is green\")}')\n","print(\n","    f'Log probability of \"we pass\": {calculate_sentence_prob_improved(\"we pass\")}')\n"]},{"cell_type":"markdown","metadata":{},"source":["The more negative a log probability is, the less likely the sentence is."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Translation modeling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[['the', 'dog', 'runs'], ['the', 'cat', 'sleeps']]\n","{('the', '<NULL>'): 1e-05, ('the', 'le'): 0.199998, ('the', 'chien'): 0.199998, ('the', 'court'): 0.199998, ('the', 'chat'): 0.199998, ('the', 'dort'): 0.199998, ('dog', '<NULL>'): 1e-05, ('dog', 'le'): 0.33333, ('dog', 'chien'): 0.33333, ('dog', 'court'): 0.33333, ('runs', '<NULL>'): 1e-05, ('runs', 'le'): 0.33333, ('runs', 'chien'): 0.33333, ('runs', 'court'): 0.33333, ('cat', '<NULL>'): 1e-05, ('cat', 'le'): 0.33333, ('cat', 'chat'): 0.33333, ('cat', 'dort'): 0.33333, ('sleeps', '<NULL>'): 1e-05, ('sleeps', 'le'): 0.33333, ('sleeps', 'chat'): 0.33333, ('sleeps', 'dort'): 0.33333, ('i', '<NULL>'): 1e-05, ('i', 'je'): 0.499995, ('i', 'suis'): 0.499995, ('am', '<NULL>'): 1e-05, ('am', 'je'): 0.499995, ('am', 'suis'): 0.499995}\n"]}],"source":["import string\n","\n","def tokenize_corpus(corpus, add_null=False):\n","    \"\"\"Tokenize the input corpus (a list of sentences) into a list of lists of tokens.\n","    Optionally add a NULL token at the beginning of each sentence.\"\"\"\n","    clean_corpus = [sentence.translate(str.maketrans(\n","        '', '', string.punctuation)) for sentence in corpus]\n","    tokenized_corpus = [sentence.lower().split() for sentence in clean_corpus]\n","    if add_null:\n","        for sentence in tokenized_corpus:\n","            sentence.insert(0, \"<NULL>\")\n","    return tokenized_corpus\n","\n","\n","def initialize_translation_prob(corpus_english, corpus_foreign):\n","    \"\"\"Initialize translation probabilities with a lower probability for NULL.\"\"\"\n","\n","    word_correspondence = {}\n","\n","    for sentence_e, sentence_f in zip(corpus_english, corpus_foreign):\n","        for word_e in sentence_e:\n","            if word_e not in word_correspondence:\n","                word_correspondence[word_e] = []\n","            for word_f in sentence_f:\n","                if word_f not in word_correspondence[word_e]:\n","                    word_correspondence[word_e] += [word_f]\n","\n","\n","    translation_prob = {}\n","    null_prob = 0.00001\n","\n","    # print(corpus_foreign)\n","\n","\n","    for word_e in word_correspondence:\n","        for word_f in word_correspondence[word_e]:\n","            if word_f == \"<NULL>\":\n","               translation_prob[(word_e, word_f)] = null_prob\n","            else:\n","                translation_prob[(word_e, word_f)] = (1 - null_prob) / (len(word_correspondence[word_e]) - 1)\n","            #translation_prob[(word_e, word_f)] = 1 / len(word_correspondence[word_e])\n","\n","    return translation_prob\n","\n","\n","\n","\n","print(tokenize_corpus([\"The dog runs\", \"The cat sleeps\"]))\n","print(initialize_translation_prob(tokenize_corpus([\"The dog runs\", \"The cat sleeps\", \"I am\"]), tokenize_corpus([\"Le chien court\", \"Le chat dort\", \"Je suis\"], add_null=True)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","\n","def ibm_model_1(corpus_english, corpus_foreign, iterations=10):\n","    # Assuming tokenize_corpus adds a \"null\" token to the beginning of each English sentence\n","    # and splits sentences into lists of words.\n","    corpus_foreign_tokens = tokenize_corpus(corpus_foreign, add_null=True)  # foreign language corpus\n","    corpus_english_tokens = tokenize_corpus(corpus_english)  # English corpus, with null word\n","\n","    # Initialize translation probabilities uniformly\n","    translation_prob = initialize_translation_prob(corpus_english_tokens, corpus_foreign_tokens)\n","\n","    for iteration in range(iterations):\n","        count_ef = defaultdict(float)\n","        total_e = defaultdict(float)\n","        \n","        # E-step: Expectation\n","        for sentence_e, sentence_f in zip(corpus_english_tokens, corpus_foreign_tokens):\n","            # For each word in the english sentence\n","            for word_f in sentence_f:\n","                # Compute normalization factor for the word2\n","                s_total_word_e = sum(translation_prob[(word_e, word_f)] for word_e in sentence_e)\n","                # For each word in the foreign sentence\n","                for word_e in sentence_e:\n","                    # Calculate delta, which is the proportion of the alignment probability of the word2 to the word1\n","                    delta = translation_prob[(word_e, word_f)] / s_total_word_e\n","                    # Update counts\n","                    count_ef[(word_e, word_f)] += delta\n","                    total_e[word_e] += delta\n","        \n","        # M-step: Maximization\n","        for (word_e, word_f), count in count_ef.items():\n","            translation_prob[(word_e, word_f)] = count / total_e[word_e]\n","\n","\n","        # normalize probabilities\n","        new_dict = {}\n","        for key, value in translation_prob.items():\n","            if key[0] not in new_dict:\n","                new_dict[key[0]] = value\n","            else:\n","                new_dict[key[0]] += value\n","\n","        for key, value in translation_prob.items():\n","            translation_prob[key] = value / new_dict[key[0]]\n","\n","    return translation_prob\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","\n","\n","def ibm_model_1_optimized(corpus_english, corpus_foreign, iterations=10):\n","    corpus_foreign_tokens = tokenize_corpus(corpus_foreign, add_null=True)\n","    corpus_english_tokens = tokenize_corpus(corpus_english)\n","\n","    translation_prob = initialize_translation_prob(\n","        corpus_english_tokens, corpus_foreign_tokens)\n","\n","    for iteration in range(iterations):\n","        count_ef = defaultdict(float)\n","        total_e = defaultdict(float)\n","\n","        # E-step: Expectation\n","        for sentence_e, sentence_f in zip(corpus_english_tokens, corpus_foreign_tokens):\n","            # Optimization: Use local variables to reduce global lookups\n","            # Convert to a list once per sentence pair, if not already a list\n","            sentence_e_local = list(sentence_e)\n","            get_translation_prob = translation_prob.get  # Local function reference\n","\n","            # For each word in the foreign sentence\n","            for word_f in sentence_f:\n","                # Optimization: Cache computed probabilities and use list comprehension for sum\n","                word_probs = [get_translation_prob(\n","                    (word_e, word_f), 0.0) for word_e in sentence_e_local]\n","                s_total_word_e = sum(word_probs)\n","\n","                # Optimization: Use enumerate to iterate over both words and cached probabilities\n","                for idx, word_e in enumerate(sentence_e_local):\n","                    delta = word_probs[idx] / s_total_word_e\n","                    count_ef[(word_e, word_f)] += delta\n","                    total_e[word_e] += delta\n","\n","        # M-step: Maximization\n","        for (word_e, word_f), count in count_ef.items():\n","            translation_prob[(word_e, word_f)] = count / total_e[word_e]\n","\n","        # Optimization: Normalize probabilities more efficiently\n","        for word_e in total_e:\n","            normalization_factor = 0\n","            for sentence_f in corpus_foreign_tokens:\n","                for word_f in sentence_f:\n","                    normalization_factor += translation_prob.get(\n","                        (word_e, word_f), 0.0)\n","            for sentence_f in corpus_foreign_tokens:\n","                for word_f in sentence_f:\n","                    pair = (word_e, word_f)\n","                    if pair in translation_prob:  # Check to avoid creating zero entries\n","                        translation_prob[pair] /= normalization_factor\n","\n","    return translation_prob\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{('the', '<NULL>'): 1e-05, ('the', 'das'): 0.33333, ('the', 'haus'): 0.33333, ('the', 'buch'): 0.33333, ('house', '<NULL>'): 1e-05, ('house', 'das'): 0.2499975, ('house', 'haus'): 0.2499975, ('house', 'ein'): 0.2499975, ('house', 'großes'): 0.2499975, ('book', '<NULL>'): 1e-05, ('book', 'das'): 0.499995, ('book', 'buch'): 0.499995, ('a', '<NULL>'): 1e-05, ('a', 'ein'): 0.33333, ('a', 'großes'): 0.33333, ('a', 'haus'): 0.33333, ('big', '<NULL>'): 1e-05, ('big', 'ein'): 0.33333, ('big', 'großes'): 0.33333, ('big', 'haus'): 0.33333}\n","Top translations for 'house':\n","haus: 0.13227183579011997\n","<NULL>: 0.02466562698421499\n","das: 5.373602007709076e-21\n","ein: 1.953105363653598e-30\n","großes: 1.953105363653598e-30\n","0.15693746277433496\n"]}],"source":["# Example usage (using dummy data):\n","corpus1 = [\"the house\", \"the book\", \"a big house\"]\n","corpus2 = [\"das haus\", \"das buch\", \"ein großes haus\"]  # Assuming German for demonstration\n","\n","print(initialize_translation_prob(tokenize_corpus(corpus1), tokenize_corpus(corpus2, add_null=True)))\n","\n","# Estimate translation probabilities\n","translation_prob = ibm_model_1_optimized(corpus1, corpus2, iterations=100)\n","\n","# Find translations for a specific word (e.g., \"house\")\n","translations_for_word = {pair[1]: prob for pair, prob in translation_prob.items() if pair[0] == \"house\"}\n","# Sort translations by probability\n","sorted_translations = sorted(translations_for_word.items(), key=lambda item: item[1], reverse=True)\n","\n","# Print top N translations\n","print(\"Top translations for 'house':\")\n","summm = 0\n","for foreign_word, prob in sorted_translations[:10]:\n","    print(f\"{foreign_word}: {prob}\")\n","    summm += prob\n","print(summm)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Need to be very careful with the NULL probability. It needs to be way lower than the other probabilities, otherwise the model will always choose the NULL translation, because NULL will be present in every sentence. Increasing the number of iterations will eventually make null the most probable translation for every word."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["test with swede"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Write code that implements the estimation algorithm for IBM model 1.\n","# Then print, for either Swedish, German, or French, the 10 words that \n","#the English word european is most likely to be translated into, according \n","#to your estimate. It can be interesting to look at this list of 10 words and\n","#see how it changes during the EM iterations.\n","\n","#reduce the size of corpus_de_en and corpus_sv_en\n","sv_en = open(en_sv_file_path, 'r', encoding='utf-8').read()\n","#separate the sentences\n","sv_en = sv_en.split(\"\\n\")\n","corpus_en = sv_en[:]\n","\n","sv = open(sv_file_path, 'r', encoding='utf-8').read()\n","#separate the sentences\n","sv = sv.split(\"\\n\")\n","corpus_sv = sv[:]\n","\n","#print(corpus_en)\n","#print(corpus_sv)\n","\n","# Estimate translation probabilities\n","translation_prob_sv = ibm_model_1(corpus_en, corpus_sv, iterations=5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#reduce the size of corpus_de_en and corpus_sv_en\n","fr_en = open(en_fr_file_path, 'r', encoding='utf-8').read()\n","#separate the sentences\n","fr_en = fr_en.split(\"\\n\")\n","corpus_en = fr_en[:]\n","\n","fr = open(fr_file_path, 'r', encoding='utf-8').read()\n","#separate the sentences\n","fr = fr.split(\"\\n\")\n","corpus_fr = fr[:]\n","\n","# Estimate translation probabilities\n","translation_prob_fr = ibm_model_1(\n","    corpus_en, corpus_fr, iterations=30)\n"]},{"cell_type":"code","execution_count":160,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top translations for 'european' in French:\n","européenne: 0.4793358348474479\n","européen: 0.2891000250623562\n","apos: 0.09885042396056481\n","l: 0.0869579716619051\n","de: 0.029416858572761516\n","au: 0.005985469766092662\n","le: 0.0037956171957049777\n","<NULL>: 0.0024198789253493296\n","en: 0.000951969306135209\n","d: 0.0007054494484298159\n"]}],"source":["# Find translations for a specific word (e.g., \"european\")\n","translations_for_word = {pair[1]: prob for pair,\n","                         prob in translation_prob_fr.items() if pair[0] == \"european\"}\n","# Sort the top 10 translations by probability\n","\n","suma = 0\n","\n","for i in translations_for_word:\n","    suma = suma + translations_for_word[i]\n","french_translations = sorted(\n","    translations_for_word.items(), key=lambda item: item[1], reverse=True)[:10]\n","\n","print(\"Top translations for 'european' in French:\")\n","\n","for foreign_word, prob in french_translations:\n","    print(f\"{foreign_word}: {prob}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["test with french"]},{"cell_type":"code","execution_count":159,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top translations for 'european' in Swedish:\n"]}],"source":["# Find translations for a specific word (e.g., \"european\")\n","translations_for_word = {pair[1]: prob for pair,\n","                         prob in translation_prob.items() if pair[0] == \"european\"}\n","# Sort the top 10 translations by probability\n","\n","suma = 0\n","\n","for i in translations_for_word:\n","    suma = suma + translations_for_word[i]\n","swedish_translations = sorted(\n","    translations_for_word.items(), key=lambda item: item[1], reverse=True)[:10]\n","\n","print(\"Top translations for 'european' in Swedish:\")\n","\n","for foreign_word, prob in swedish_translations:\n","    print(f\"{foreign_word}: {prob}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Decoding"]},{"cell_type":"code","execution_count":172,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["word list for je: [('i', 0.6624285821735717), ('rossa', 0.27582155226175936), ('am', 0.23761432290720727), ('like', 0.22266832182209578), ('please', 0.20905968207564107)]\n","\n","\n","word list for suis: [('am', 0.40911230518245667), ('galicia', 0.2430798650512864), ('contacted', 0.20017739757920394), ('abstained', 0.1884958628197376), ('campaigning', 0.1835540759633998)]\n","\n","\n","word list for européenne: [('european', 0.4793358348474479), ('reluctance', 0.17700219884502422), ('construction', 0.16067392674243278), ('periodical', 0.15517872071109737), ('harsh', 0.1506930590885051)]\n","\n","\n","French sentence: je suis européenne\n","Translated sentence: i am european (log probability: -2.0409619139049204)\n"]}],"source":["# def translate_sentence(sentence, translation_prob, n_words=5):\n","#     \"\"\"\n","#     Translate one sentence from a foreign language to English.\n","#     In the algorithm, we will keep the top n word translations for \n","#     each word in the sentence. Moreover, we will use beam search to\n","#     keep the most likely translations for the whole sentence.\n","#     \"\"\"\n","\n","#     translations = []\n","#     top_n_word_translations = get_top_n_word_translations(\n","#         sentence, translation_prob, n_words)\n","    \n","#     # make a combination of every single possibility, \n","#     # maintaining the order of the words\n","#     for word in sentence:\n","#         if word in top_n_word_translations:\n","#             if len(translations) == 0:\n","#                 for translation in top_n_word_translations[word]:\n","#                     translations.append([translation])\n","#             else:\n","#                 temp_translations = []\n","#                 for sentence in translations:\n","#                     for translation in top_n_word_translations[word]:\n","\n","#                         temp_translations.append(sentence + [translation])\n","#                 translations = temp_translations\n","\n","\n","#     highest_prob_sentence = []\n","#     highest_prob = -10000000\n","#     for sentence in translations:\n","#         sentence_probability = calculate_sentence_prob_improved(\n","#             \" \".join([word[0] for word in sentence]))\n","#         if sentence_probability > highest_prob:\n","#             highest_prob = sentence_probability\n","#             highest_prob_sentence = sentence\n","\n","#     return \" \".join([word[0] for word in highest_prob_sentence])\n","\n","import heapq\n","\n","\n","def get_top_n_word_translations(foreign_sentence, translation_prob, n):\n","    word_translations = {}\n","    for word in foreign_sentence:\n","        #get top 5 translations for each word\n","        translations_for_word = {\n","            pair[0]: prob for pair, prob \n","            in translation_prob.items() if pair[1] == word}\n","        # Sort translations by probability\n","        sorted_translations = sorted(\n","            translations_for_word.items(), \n","            key=lambda item: item[1], reverse=True)[:n]\n","        word_translations[word] = sorted_translations\n","        print(f'word list for {word}: {word_translations[word]}')\n","        print('\\n')\n","    \n","    return word_translations\n","\n","def translate_sentence_approx(sentence, translation_prob, \n","                              n_words=5, beam_width=5):\n","    \"\"\"\n","    Translate one sentence from a foreign language to English.\n","    In the algorithm, we will keep the top n word translations for \n","    each word in the sentence. Moreover, we will use beam search to\n","    keep the most likely translations for the whole sentence.\n","    \"\"\"\n","    \n","    beam = [(0, [])]  # (log_prob, sequence)\n","    top_n_word_translations = get_top_n_word_translations(\n","        sentence, translation_prob, n_words)\n","\n","    # Iterate over each word in the foreign sentence\n","    for word in sentence:\n","        # Get the top translations for the current foreign word\n","        if word in top_n_word_translations:\n","            top_translations = top_n_word_translations[word]\n","        else:\n","            continue\n","\n","        next_beam = []\n","\n","        # Expand each sequence in the beam with \n","        # each translation of the current foreign word\n","        for log_prob, seq in beam:\n","            for (translation, translation_prob) in top_translations:\n","                new_seq = seq + [translation]\n","                \n","                # Update the log probability\n","                new_log_prob = log_prob + math.log(translation_prob)\n","                next_beam.append((new_log_prob, new_seq))\n","                \n","        # Keep only the top `beam_width` sequences\n","        beam = heapq.nlargest(beam_width, next_beam, key=lambda x: x[0])\n","        \n","    prob, highest_prob_sentence = (0, '') if len(beam) == 0 else max(beam, key=lambda x: x[0])\n","    return prob, \" \".join(highest_prob_sentence)\n","\n","\n","french_sentence = \"je suis européenne\".split()\n","prob_translated_sentennce, translated_sentence = translate_sentence_approx(\n","    sentence=french_sentence,  translation_prob=translation_prob_fr, \n","    n_words=5)\n","print(f\"French sentence: {' '.join(french_sentence)}\")\n","print(\n","    f\"Translated sentence: {translated_sentence} (log probability: {prob_translated_sentennce})\")\n"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"a97e5a613ce44d2bbe2564741918b105","deepnote_persisted_session":{"createdAt":"2024-01-29T14:08:51.714Z"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"vscode":{"interpreter":{"hash":"8e21d9f50f7c30e7092d9e7e50ded244397ed7adf91fc30361913d3d8a47365c"}}},"nbformat":4,"nbformat_minor":0}
